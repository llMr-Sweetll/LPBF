{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LPBF Optimizer Workflow Example\n",
    "\n",
    "This notebook demonstrates the complete workflow of the physics-informed, AI-driven optimizer for Laser Powder Bed Fusion (LPBF) manufacturing processes. We'll walk through each step from FEA simulation to optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "# Add parent directory to path to import local modules\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Import project modules\n",
    "from src.pinn.model import PINN\n",
    "from src.pinn.physics import compute_physics_loss\n",
    "from src.pinn.train import PINNTrainer\n",
    "from src.optimiser.nsga3 import NSGAOptimizer\n",
    "from src.optimiser.bayesopt import BayesianOptimizer\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "First, let's load our configuration from the params.yaml file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config_path = Path('../data/params.yaml')\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Show material properties\n",
    "print(\"Material Properties (Ti-6Al-4V):\")\n",
    "for key, value in config['material_properties'].items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Exploration\n",
    "\n",
    "Next, let's explore the FEA simulation data. In a real scenario, this would be loaded from FEA simulation results. For this demo, we'll create some synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some synthetic FEA data for demonstration\n",
    "def generate_synthetic_data(n_samples=1000):\n",
    "    \"\"\"\n",
    "    Generate synthetic data that mimics FEA simulation results\n",
    "    \"\"\"\n",
    "    # Process parameters\n",
    "    P = np.random.uniform(150, 400, n_samples)  # Laser power (W)\n",
    "    v = np.random.uniform(500, 1500, n_samples)  # Scan speed (mm/s)\n",
    "    h = np.random.uniform(0.05, 0.15, n_samples)  # Hatch spacing (mm)\n",
    "    theta = np.random.uniform(0, 90, n_samples)  # Scan angle (degrees)\n",
    "    l_island = np.random.uniform(2, 10, n_samples)  # Island size (mm)\n",
    "    layer_thickness = np.random.uniform(0.02, 0.06, n_samples)  # Layer thickness (mm)\n",
    "    \n",
    "    # Spatial coordinates\n",
    "    x = np.random.uniform(-5, 5, n_samples)\n",
    "    y = np.random.uniform(-5, 5, n_samples)\n",
    "    z = np.random.uniform(0, 1, n_samples)\n",
    "    t = np.random.uniform(0, 1, n_samples)  # Time\n",
    "    \n",
    "    # Create input array: [P, v, h, theta, l_island, layer_thickness, x, y, z, t]\n",
    "    inputs = np.column_stack([P, v, h, theta, l_island, layer_thickness, x, y, z, t])\n",
    "    \n",
    "    # Generate synthetic outputs\n",
    "    # Residual stress (MPa) - increases with P, decreases with v\n",
    "    residual_stress = 200 + 0.5*P - 0.1*v + 50*np.random.randn(n_samples)\n",
    "    \n",
    "    # Porosity (%) - decreases with Energy Density (P/(v*h))\n",
    "    energy_density = P / (v * h)\n",
    "    porosity = 5 - 0.1*np.log(energy_density) + 1*np.random.randn(n_samples)\n",
    "    porosity = np.clip(porosity, 0.1, 10) / 100  # Convert to fraction and clip to reasonable range\n",
    "    \n",
    "    # Geometric accuracy (dimensionless) - better with moderate energy density\n",
    "    geo_accuracy = 0.9 + 0.1*np.exp(-(energy_density - 0.3)**2 / 0.1) + 0.02*np.random.randn(n_samples)\n",
    "    geo_accuracy = np.clip(geo_accuracy, 0.7, 1.0)  # Clip to reasonable range\n",
    "    \n",
    "    # Combine outputs\n",
    "    outputs = np.column_stack([residual_stress, porosity, geo_accuracy])\n",
    "    \n",
    "    return inputs, outputs, np.column_stack([x, y, z]), t.reshape(-1, 1)\n",
    "\n",
    "# Generate data\n",
    "inputs, outputs, coords, times = generate_synthetic_data(5000)\n",
    "\n",
    "# Split into train/val/test\n",
    "indices = np.random.permutation(len(inputs))\n",
    "n_train = int(0.8 * len(indices))\n",
    "n_val = int(0.1 * len(indices))\n",
    "\n",
    "train_indices = indices[:n_train]\n",
    "val_indices = indices[n_train:n_train+n_val]\n",
    "test_indices = indices[n_train+n_val:]\n",
    "\n",
    "train_inputs, train_outputs = inputs[train_indices], outputs[train_indices]\n",
    "val_inputs, val_outputs = inputs[val_indices], outputs[val_indices]\n",
    "test_inputs, test_outputs = inputs[test_indices], outputs[test_indices]\n",
    "\n",
    "# Create a synthetic dataset file\n",
    "data_file = Path('../data/processed/synthetic_dataset.h5')\n",
    "data_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with h5py.File(data_file, 'w') as f:\n",
    "    # Training data\n",
    "    train_group = f.create_group('train')\n",
    "    train_group.create_dataset('inputs', data=train_inputs)\n",
    "    train_group.create_dataset('outputs', data=train_outputs)\n",
    "    train_group.create_dataset('coordinates', data=coords[train_indices])\n",
    "    train_group.create_dataset('time', data=times[train_indices])\n",
    "    \n",
    "    # Extract scan vectors (just the process parameters)\n",
    "    train_group.create_dataset('scan_vectors', data=train_inputs[:, :6])\n",
    "    \n",
    "    # Validation data\n",
    "    val_group = f.create_group('val')\n",
    "    val_group.create_dataset('inputs', data=val_inputs)\n",
    "    val_group.create_dataset('outputs', data=val_outputs)\n",
    "    val_group.create_dataset('coordinates', data=coords[val_indices])\n",
    "    val_group.create_dataset('time', data=times[val_indices])\n",
    "    val_group.create_dataset('scan_vectors', data=val_inputs[:, :6])\n",
    "    \n",
    "    # Test data\n",
    "    test_group = f.create_group('test')\n",
    "    test_group.create_dataset('inputs', data=test_inputs)\n",
    "    test_group.create_dataset('outputs', data=test_outputs)\n",
    "    test_group.create_dataset('coordinates', data=coords[test_indices])\n",
    "    test_group.create_dataset('time', data=times[test_indices])\n",
    "    test_group.create_dataset('scan_vectors', data=test_inputs[:, :6])\n",
    "    \n",
    "    # Metadata\n",
    "    meta_group = f.create_group('metadata')\n",
    "    meta_group.attrs['n_total'] = len(inputs)\n",
    "    meta_group.attrs['n_train'] = len(train_indices)\n",
    "    meta_group.attrs['n_val'] = len(val_indices)\n",
    "    meta_group.attrs['n_test'] = len(test_indices)\n",
    "    \n",
    "    # Parameter names\n",
    "    dt = h5py.special_dtype(vlen=str)\n",
    "    param_names = np.array(['P', 'v', 'h', 'theta', 'l_island', 'layer_thickness'], dtype=object)\n",
    "    meta_group.create_dataset('parameter_names', data=param_names, dtype=dt)\n",
    "\n",
    "print(f\"Created synthetic dataset at {data_file} with {len(inputs)} samples\")\n",
    "print(f\"  Training: {len(train_indices)} samples\")\n",
    "print(f\"  Validation: {len(val_indices)} samples\")\n",
    "print(f\"  Test: {len(test_indices)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize some of the relationships in our synthetic data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize relationship between process parameters and outputs\n",
    "fig, axs = plt.subplots(3, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot residual stress vs. power and speed\n",
    "axs[0, 0].scatter(inputs[:, 0], outputs[:, 0], alpha=0.3)\n",
    "axs[0, 0].set_xlabel('Laser Power (W)')\n",
    "axs[0, 0].set_ylabel('Residual Stress (MPa)')\n",
    "axs[0, 0].set_title('Residual Stress vs. Laser Power')\n",
    "\n",
    "axs[0, 1].scatter(inputs[:, 1], outputs[:, 0], alpha=0.3)\n",
    "axs[0, 1].set_xlabel('Scan Speed (mm/s)')\n",
    "axs[0, 1].set_ylabel('Residual Stress (MPa)')\n",
    "axs[0, 1].set_title('Residual Stress vs. Scan Speed')\n",
    "\n",
    "# Plot porosity vs. power and speed\n",
    "axs[1, 0].scatter(inputs[:, 0], outputs[:, 1]*100, alpha=0.3)  # Convert back to percentage\n",
    "axs[1, 0].set_xlabel('Laser Power (W)')\n",
    "axs[1, 0].set_ylabel('Porosity (%)')\n",
    "axs[1, 0].set_title('Porosity vs. Laser Power')\n",
    "\n",
    "axs[1, 1].scatter(inputs[:, 1], outputs[:, 1]*100, alpha=0.3)  # Convert back to percentage\n",
    "axs[1, 1].set_xlabel('Scan Speed (mm/s)')\n",
    "axs[1, 1].set_ylabel('Porosity (%)')\n",
    "axs[1, 1].set_title('Porosity vs. Scan Speed')\n",
    "\n",
    "# Plot geometric accuracy vs. power and speed\n",
    "axs[2, 0].scatter(inputs[:, 0], outputs[:, 2], alpha=0.3)\n",
    "axs[2, 0].set_xlabel('Laser Power (W)')\n",
    "axs[2, 0].set_ylabel('Geometric Accuracy')\n",
    "axs[2, 0].set_title('Geometric Accuracy vs. Laser Power')\n",
    "\n",
    "axs[2, 1].scatter(inputs[:, 1], outputs[:, 2], alpha=0.3)\n",
    "axs[2, 1].set_xlabel('Scan Speed (mm/s)')\n",
    "axs[2, 1].set_ylabel('Geometric Accuracy')\n",
    "axs[2, 1].set_title('Geometric Accuracy vs. Scan Speed')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Also show energy density plots (P/vh)\n",
    "energy_density = inputs[:, 0] / (inputs[:, 1] * inputs[:, 2])\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "axs[0].scatter(energy_density, outputs[:, 0], alpha=0.3)\n",
    "axs[0].set_xlabel('Energy Density (J/mm³)')\n",
    "axs[0].set_ylabel('Residual Stress (MPa)')\n",
    "axs[0].set_title('Residual Stress vs. Energy Density')\n",
    "\n",
    "axs[1].scatter(energy_density, outputs[:, 1]*100, alpha=0.3)  # Convert back to percentage\n",
    "axs[1].set_xlabel('Energy Density (J/mm³)')\n",
    "axs[1].set_ylabel('Porosity (%)')\n",
    "axs[1].set_title('Porosity vs. Energy Density')\n",
    "\n",
    "axs[2].scatter(energy_density, outputs[:, 2], alpha=0.3)\n",
    "axs[2].set_xlabel('Energy Density (J/mm³)')\n",
    "axs[2].set_ylabel('Geometric Accuracy')\n",
    "axs[2].set_title('Geometric Accuracy vs. Energy Density')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PINN Model\n",
    "\n",
    "Now let's create and train our Physics-Informed Neural Network (PINN) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PINN model\n",
    "model = PINN(\n",
    "    in_dim=config['model']['input_dim'],\n",
    "    out_dim=config['model']['output_dim'],\n",
    "    width=config['model']['hidden_width'],\n",
    "    depth=config['model']['hidden_depth']\n",
    ")\n",
    "\n",
    "print(\"PINN Model Architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the training process\n",
    "# For the notebook demo, we'll use a simplified training setup\n",
    "config['training']['n_epochs'] = 50  # Reduce epochs for the demo\n",
    "config['training']['batch_size'] = 64\n",
    "config['training']['print_freq'] = 5\n",
    "config['data']['processed_data_path'] = '../data/processed/synthetic_dataset.h5'\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = PINNTrainer(config_path)\n",
    "\n",
    "# Load data for training\n",
    "trainer.load_data()\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting PINN training (this may take a while)...\")\n",
    "trainer.train()\n",
    "\n",
    "# Plot training metrics\n",
    "trainer.plot_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation\n",
    "\n",
    "Let's evaluate our trained model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "best_model_path = list(trainer.checkpoint_dir.glob('best_model.pt'))[0]\n",
    "trainer.load_checkpoint(best_model_path)\n",
    "\n",
    "# Evaluate on test data\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "test_data = torch.tensor(test_inputs, dtype=torch.float32).to(device)\n",
    "test_labels = torch.tensor(test_outputs, dtype=torch.float32).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    predictions = model(test_data)\n",
    "\n",
    "# Convert to numpy for easier analysis\n",
    "predictions = predictions.cpu().numpy()\n",
    "test_labels = test_labels.cpu().numpy()\n",
    "\n",
    "# Calculate metrics for each output\n",
    "mse = np.mean((predictions - test_labels)**2, axis=0)\n",
    "mae = np.mean(np.abs(predictions - test_labels), axis=0)\n",
    "r2 = 1 - np.sum((predictions - test_labels)**2, axis=0) / np.sum((test_labels - np.mean(test_labels, axis=0))**2, axis=0)\n",
    "\n",
    "print(\"Test set evaluation:\")\n",
    "output_names = ['Residual Stress', 'Porosity', 'Geometric Accuracy']\n",
    "for i, name in enumerate(output_names):\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  MSE: {mse[i]:.4f}\")\n",
    "    print(f\"  MAE: {mae[i]:.4f}\")\n",
    "    print(f\"  R²: {r2[i]:.4f}\")\n",
    "\n",
    "# Plot predicted vs. actual for each output\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for i, (name, ax) in enumerate(zip(output_names, axs)):\n",
    "    ax.scatter(test_labels[:, i], predictions[:, i], alpha=0.3)\n",
    "    \n",
    "    # Add diagonal line (perfect prediction)\n",
    "    min_val = min(np.min(test_labels[:, i]), np.min(predictions[:, i]))\n",
    "    max_val = max(np.max(test_labels[:, i]), np.max(predictions[:, i]))\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "    \n",
    "    ax.set_xlabel(f'Actual {name}')\n",
    "    ax.set_ylabel(f'Predicted {name}')\n",
    "    ax.set_title(f'{name}: R² = {r2[i]:.4f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multi-Objective Optimization\n",
    "\n",
    "Now that we have a trained surrogate model, let's use it for multi-objective optimization using NSGA-III."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the optimizer\n",
    "optimizer = NSGAOptimizer(config_path, best_model_path)\n",
    "\n",
    "# Run the optimization\n",
    "print(\"Running NSGA-III optimization (this may take a while)...\")\n",
    "results = optimizer.optimize()\n",
    "\n",
    "# Extract Pareto front solutions\n",
    "pareto_params = results.X  # Process parameters on the Pareto front\n",
    "pareto_objectives = results.F  # Corresponding objective values\n",
    "\n",
    "print(f\"Optimization complete with {len(pareto_params)} solutions on the Pareto front\")\n",
    "\n",
    "# Plot the Pareto front\n",
    "if pareto_objectives.shape[1] == 2:\n",
    "    # 2D Pareto front\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(pareto_objectives[:, 0], pareto_objectives[:, 1], s=30)\n",
    "    plt.xlabel(config['optimizer']['objectives'][0])\n",
    "    plt.ylabel(config['optimizer']['objectives'][1])\n",
    "    plt.title('Pareto Front')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "elif pareto_objectives.shape[1] == 3:\n",
    "    # 3D Pareto front\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(pareto_objectives[:, 0], pareto_objectives[:, 1], pareto_objectives[:, 2], s=30)\n",
    "    ax.set_xlabel(config['optimizer']['objectives'][0])\n",
    "    ax.set_ylabel(config['optimizer']['objectives'][1])\n",
    "    ax.set_zlabel(config['optimizer']['objectives'][2])\n",
    "    ax.set_title('Pareto Front')\n",
    "    plt.show()\n",
    "\n",
    "# Display some of the optimal solutions\n",
    "param_names = list(config['optimizer']['param_bounds'].keys())\n",
    "objective_names = config['optimizer']['objectives']\n",
    "\n",
    "print(\"\\nSample optimal solutions from the Pareto front:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Solution':^10} | \" + \" | \".join([f\"{name:^12}\" for name in param_names]) + \" | \" + \n",
    "      \" | \".join([f\"{name:^18}\" for name in objective_names]))\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Sample solutions (every 10th solution or fewer if there are fewer than 50 solutions)\n",
    "step = max(1, len(pareto_params) // 5)\n",
    "for i in range(0, len(pareto_params), step):\n",
    "    param_str = \" | \".join([f\"{val:12.4f}\" for val in pareto_params[i]])\n",
    "    obj_str = \" | \".join([f\"{val:18.4f}\" for val in pareto_objectives[i]])\n",
    "    print(f\"{i:^10} | {param_str} | {obj_str}\")\n",
    "\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the trade-offs between objectives\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Apply PCA to process parameters to find primary directions of variation\n",
    "pca = PCA(n_components=2)\n",
    "params_pca = pca.fit_transform(pareto_params)\n",
    "\n",
    "# Create scatter plot colored by each objective\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for i, (name, ax) in enumerate(zip(objective_names, axs)):\n",
    "    scatter = ax.scatter(params_pca[:, 0], params_pca[:, 1], \n",
    "                         c=pareto_objectives[:, i], cmap='viridis', \n",
    "                         s=50, alpha=0.8)\n",
    "    ax.set_xlabel('PCA Component 1')\n",
    "    ax.set_ylabel('PCA Component 2')\n",
    "    ax.set_title(f'Parameter Space Colored by {name}')\n",
    "    fig.colorbar(scatter, ax=ax, label=name)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display PCA component weights\n",
    "print(\"PCA Component Weights (Contribution of each parameter to the principal components):\")\n",
    "for i, comp in enumerate(pca.components_):\n",
    "    print(f\"Component {i+1}:\")\n",
    "    for name, weight in zip(param_names, comp):\n",
    "        print(f\"  {name}: {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Selecting Optimal Process Parameters\n",
    "\n",
    "Based on the Pareto front, let's select a few process parameter sets for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select solutions with desired properties\n",
    "# For example, select solutions with minimal residual stress\n",
    "min_stress_idx = np.argmin(pareto_objectives[:, 0])\n",
    "min_stress_params = pareto_params[min_stress_idx]\n",
    "min_stress_objectives = pareto_objectives[min_stress_idx]\n",
    "\n",
    "# Minimal porosity\n",
    "min_porosity_idx = np.argmin(pareto_objectives[:, 1])\n",
    "min_porosity_params = pareto_params[min_porosity_idx]\n",
    "min_porosity_objectives = pareto_objectives[min_porosity_idx]\n",
    "\n",
    "# Maximum geometric accuracy\n",
    "# Assuming here that objective is negative geometric accuracy (to be minimized)\n",
    "max_geo_idx = np.argmin(pareto_objectives[:, 2])  \n",
    "max_geo_params = pareto_params[max_geo_idx]\n",
    "max_geo_objectives = pareto_objectives[max_geo_idx]\n",
    "\n",
    "# Balanced solution (middle of Pareto front)\n",
    "# One approach: find solution with minimum Euclidean distance to the utopia point\n",
    "# First normalize objectives to [0,1] scale\n",
    "obj_min = np.min(pareto_objectives, axis=0)\n",
    "obj_max = np.max(pareto_objectives, axis=0)\n",
    "obj_range = obj_max - obj_min\n",
    "normalized_objectives = (pareto_objectives - obj_min) / obj_range\n",
    "\n",
    "# Utopia point is [0,0,0] in normalized space\n",
    "distances = np.sqrt(np.sum(normalized_objectives**2, axis=1))\n",
    "balanced_idx = np.argmin(distances)\n",
    "balanced_params = pareto_params[balanced_idx]\n",
    "balanced_objectives = pareto_objectives[balanced_idx]\n",
    "\n",
    "# Display the selected solutions\n",
    "print(\"\\nSelected Solutions for Validation:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Criterion':^20} | \" + \" | \".join([f\"{name:^12}\" for name in param_names]) + \" | \" + \n",
    "      \" | \".join([f\"{name:^18}\" for name in objective_names]))\n",
    "print(\"-\" * 80)\n",
    "\n",
    "solutions = [\n",
    "    (\"Min Residual Stress\", min_stress_params, min_stress_objectives),\n",
    "    (\"Min Porosity\", min_porosity_params, min_porosity_objectives),\n",
    "    (\"Max Geo Accuracy\", max_geo_params, max_geo_objectives),\n",
    "    (\"Balanced\", balanced_params, balanced_objectives)\n",
    "]\n",
    "\n",
    "for name, params, objectives in solutions:\n",
    "    param_str = \" | \".join([f\"{val:12.4f}\" for val in params])\n",
    "    obj_str = \" | \".join([f\"{val:18.4f}\" for val in objectives])\n",
    "    print(f\"{name:^20} | {param_str} | {obj_str}\")\n",
    "\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Save selected parameters for validation\n",
    "validation_file = Path('../data/optimized/validation_parameters.h5')\n",
    "validation_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with h5py.File(validation_file, 'w') as f:\n",
    "    # Save parameters\n",
    "    all_params = np.vstack([params for _, params, _ in solutions])\n",
    "    f.create_dataset('parameters', data=all_params)\n",
    "    \n",
    "    # Save objectives\n",
    "    all_objectives = np.vstack([obj for _, _, obj in solutions])\n",
    "    f.create_dataset('objectives', data=all_objectives)\n",
    "    \n",
    "    # Save parameter and objective names\n",
    "    dt = h5py.special_dtype(vlen=str)\n",
    "    param_names_array = np.array(param_names, dtype=object)\n",
    "    obj_names_array = np.array(objective_names, dtype=object)\n",
    "    solution_names = np.array([name for name, _, _ in solutions], dtype=object)\n",
    "    \n",
    "    f.create_dataset('parameter_names', data=param_names_array, dtype=dt)\n",
    "    f.create_dataset('objective_names', data=obj_names_array, dtype=dt)\n",
    "    f.create_dataset('solution_names', data=solution_names, dtype=dt)\n",
    "\n",
    "print(f\"Saved validation parameters to {validation_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "In this notebook, we've demonstrated the complete workflow for physics-informed optimization of LPBF process parameters:\n",
    "\n",
    "1. Generated synthetic data that mimics FEA simulation results\n",
    "2. Trained a physics-informed neural network (PINN) as a surrogate model\n",
    "3. Validated the model on test data\n",
    "4. Used NSGA-III multi-objective optimization to find the Pareto front\n",
    "5. Selected optimal parameter sets for different objectives\n",
    "\n",
    "The next steps would be to:\n",
    "\n",
    "1. Build physical coupons using the optimized parameters\n",
    "2. Characterize the resulting parts (porosity, residual stress, geometric accuracy)\n",
    "3. Compare the experimental results to model predictions\n",
    "4. Refine the model with the new experimental data\n",
    "\n",
    "This completes the feedback loop, allowing for continuous improvement of the surrogate model and optimization process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}